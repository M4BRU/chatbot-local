# ü§ñ Brief pour Claude Code - Chatbot RAG Local VLM Robotics

## üìã Contexte

Je veux cr√©er un chatbot RAG 100% local pour assister sur des t√¢ches commerciales li√©es √† des machines robotiques (bras robotiques, fabrication additive).

### Environnement actuel

**Syst√®me** : WSL2 Ubuntu sur Windows  
**Dossier projet** : `~/chatbot-local`  
**Python** : 3.12 (venv activ√© dans `./venv`)  
**Mod√®le LLM** : Ollama avec `llama3.2:3b` (d√©j√† install√© et fonctionnel)

### Packages d√©j√† install√©s
```bash
ollama
chromadb
langchain
langchain-community
streamlit
PyPDF2
python-docx
pdf2image
pytesseract
Pillow
```

### Documents √† indexer

**Emplacement** : `~/chatbot-local/documents/`  
**Type** : 4 PDFs de brochures techniques VLM Robotics
- Plaquette-FR-GEMINI.pdf
- Plaquette-SOLO-FR.pdf
- Plaquette-FR-COMPAQT.pdf
- PLAQUETTE-HYMANCO-DEF.pdf

**Contenu** : Specs techniques, dimensions, capacit√©s, applications des machines

---

## üéØ Objectifs

### 1. Script d'ingestion : `ingest.py`

**Fonctionnalit√©s requises :**
- Lire tous les PDFs du dossier `./documents`
- Extraire le texte (PyPDF2 + OCR si n√©cessaire avec pytesseract)
- Chunking intelligent avec `RecursiveCharacterTextSplitter`
  - chunk_size : 500-800 caract√®res
  - chunk_overlap : 50-100 caract√®res
- Cr√©er des embeddings avec Ollama (`llama3.2:3b`)
- Stocker dans ChromaDB local (`./chroma_db`)
- Afficher une barre de progression
- G√©rer les erreurs proprement

**Exemple d'utilisation :**
```bash
python ingest.py
# Output : "‚úÖ 4 documents index√©s, 127 chunks cr√©√©s"
```

---

### 2. Application Streamlit : `app.py`

**Interface requise :**
- Design clean et pro (pas trop de couleurs)
- Titre : "Assistant Commercial VLM Robotics"
- Zone de chat avec historique de conversation
- Input utilisateur en bas

**Fonctionnalit√©s :**
- Charger ChromaDB au d√©marrage
- Pour chaque question :
  1. Recherche de similarit√© (top k=3-5 chunks)
  2. Construction du prompt avec contexte
  3. Appel √† Ollama `llama3.2:3b` via API
  4. Streaming de la r√©ponse si possible
  5. Affichage des sources utilis√©es (nom du fichier + page si dispo)
- Bouton "Effacer l'historique"
- Gestion des erreurs (Ollama non d√©marr√©, ChromaDB vide, etc.)

**Prompt syst√®me sugg√©r√© :**
```
Tu es un assistant commercial expert pour VLM Robotics. 
Tu aides √† r√©diger des offres et renseigner les clients sur nos machines de fabrication additive et robotique.

Contexte disponible :
{context}

Question client : {question}

R√©ponds de mani√®re professionnelle, pr√©cise et concise. 
Cite toujours la source (nom de la machine/brochure).
Si l'info n'est pas dans le contexte, dis-le clairement.
```

---

### 3. README.md

**Contenu :**
- Description du projet
- Pr√©requis
- Installation
- Utilisation
  - Comment lancer Ollama
  - Comment indexer les documents
  - Comment lancer l'app
- Exemples de questions √† poser
- Troubleshooting

---

## üîê Contraintes importantes

1. **100% local** - Aucun appel API externe (sauf Ollama local)
2. **Donn√©es s√©curis√©es** - Tout reste sur la machine
3. **Performance** - Doit fonctionner sur CPU (pas de GPU requis)
4. **Simplicit√©** - Code lisible et maintenable
5. **Fran√ßais** - Interface et prompts en fran√ßais

---

## üìù Structure de fichiers attendue
```
~/chatbot-local/
‚îú‚îÄ‚îÄ venv/                    # Environnement virtuel (existant)
‚îú‚îÄ‚îÄ documents/               # PDFs sources (existant)
‚îÇ   ‚îú‚îÄ‚îÄ Plaquette-FR-GEMINI.pdf
‚îÇ   ‚îú‚îÄ‚îÄ Plaquette-SOLO-FR.pdf
‚îÇ   ‚îú‚îÄ‚îÄ Plaquette-FR-COMPAQT.pdf
‚îÇ   ‚îî‚îÄ‚îÄ PLAQUETTE-HYMANCO-DEF.pdf
‚îú‚îÄ‚îÄ chroma_db/              # Base vectorielle (√† cr√©er)
‚îú‚îÄ‚îÄ ingest.py               # Script d'indexation (√† cr√©er)
‚îú‚îÄ‚îÄ app.py                  # Interface Streamlit (√† cr√©er)
‚îú‚îÄ‚îÄ README.md               # Documentation (√† cr√©er)
‚îî‚îÄ‚îÄ requirements.txt        # D√©pendances (optionnel)
```

---

## üß™ Exemples de questions √† supporter

- "Quels sont les diff√©rents mod√®les de machines disponibles ?"
- "Quelle est la capacit√© maximale du mod√®le GEMINI ?"
- "Compare SOLO et COMPAQT en termes de dimensions"
- "Quelle machine recommandes-tu pour fabriquer une pi√®ce de 4 m√®tres ?"
- "Qu'est-ce que la technologie WAAM ?"
- "Aide-moi √† r√©diger une offre pour un client qui veut faire de la fabrication additive XXL"

---

## ‚ö° Points d'attention

- **Ollama doit √™tre lanc√©** : `ollama serve` en arri√®re-plan
- **V√©rifier la connexion** : http://localhost:11434
- **Embeddings** : Utiliser le m√™me mod√®le que le LLM (`llama3.2:3b`)
- **Gestion m√©moire** : Attention au context window (~8k tokens pour llama3.2)
- **Erreurs communes** :
  - Ollama pas d√©marr√©
  - ChromaDB vide (pas d'ingestion faite)
  - PDFs corrompus

---

## üöÄ Commandes pour tester apr√®s cr√©ation
```bash
# 1. Activer l'environnement
source venv/bin/activate

# 2. V√©rifier Ollama
ollama list

# 3. Indexer les documents
python ingest.py

# 4. Lancer l'app
streamlit run app.py
```

---

## üìä M√©triques de succ√®s

‚úÖ Ingestion de 4 PDFs en < 2 minutes  
‚úÖ R√©ponses pertinentes avec sources cit√©es  
‚úÖ Interface fluide et r√©active  
‚úÖ Pas de crash m√™me si Ollama red√©marre  
‚úÖ Code propre et comment√©

---

## üí° Bonus (si le temps le permet)

- Possibilit√© d'ajouter des PDFs sans tout r√©indexer
- Export des conversations en PDF
- Statistiques d'utilisation
- Mode "comparaison de produits" optimis√©
- Templates d'offres commerciales

---

**Cr√©e les fichiers `ingest.py`, `app.py` et `README.md` en suivant ces specs. Merci !**
